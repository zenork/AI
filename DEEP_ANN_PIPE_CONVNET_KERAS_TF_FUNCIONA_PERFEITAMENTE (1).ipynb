{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEEP_ANN_PIPE_CONVNET_KERAS_TF_FUNCIONA_PERFEITAMENTE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbnrov6BAsPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "import cv2   #to resize the image\n",
        "import os  # to play with directories\n",
        "import numpy as np\n",
        "from random import shuffle # to shuffle the data\n",
        "from tqdm import tqdm #for professional looping with a progress bar (to see the iterations per second and the time running)\n",
        "#esse tqdm só é útil se o seu loop estiver demorando, daí vc pode ver o progresso dele\n",
        "\n",
        "\n",
        "#o pré-processamento de imagens requer carregar as imagens e organizar elas, cada imagem\n",
        "#tem um conjunto de atributos(features) e de legenda(label)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plzmIX_3BCBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SIZE = 40 # tamanho de resolução da imagem 2D em um dos eixos, aqui eu assumi que a imagem seria em formato de um quadrado.\n",
        "N_img = 50 # Quantidade de imagens que você irá carregar para treinar a rede, certifique-se de digitar o número com a quantidade EXATA.\n",
        "\n",
        "LR =1e-2 #learning rate of NN\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i9bw62DBClW",
        "colab_type": "code",
        "outputId": "9ca9b05d-d806-4f0c-a7c9-19cbc34f13b2",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1738
        }
      },
      "source": [
        "from google.colab import files\n",
        "def getLocalFiles():\n",
        "    _files = files.upload()\n",
        "    if len(_files) >0:\n",
        "       for k,v in _files.items():\n",
        "         open(k,'wb').write(v)\n",
        "getLocalFiles()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fa48ac20-0348-4dc4-9457-dce163507191\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-fa48ac20-0348-4dc4-9457-dce163507191\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving tubo_1_ANN.jpg to tubo_1_ANN.jpg\n",
            "Saving tubo_2_ANN.jpg to tubo_2_ANN.jpg\n",
            "Saving tubo_3_ANN.jpg to tubo_3_ANN.jpg\n",
            "Saving tubo_4_ANN.jpg to tubo_4_ANN.jpg\n",
            "Saving tubo_5_ANN.jpg to tubo_5_ANN.jpg\n",
            "Saving tubo_6_ANN.jpg to tubo_6_ANN.jpg\n",
            "Saving tubo_7_ANN.jpg to tubo_7_ANN.jpg\n",
            "Saving tubo_8_ANN.jpg to tubo_8_ANN.jpg\n",
            "Saving tubo_9_ANN.jpg to tubo_9_ANN.jpg\n",
            "Saving tubo_10_ANN.jpg to tubo_10_ANN.jpg\n",
            "Saving tubo_11_ANN.jpg to tubo_11_ANN.jpg\n",
            "Saving tubo_12_ANN.jpg to tubo_12_ANN.jpg\n",
            "Saving tubo_13_ANN.jpg to tubo_13_ANN.jpg\n",
            "Saving tubo_14_ANN.jpg to tubo_14_ANN.jpg\n",
            "Saving tubo_15_ANN.jpg to tubo_15_ANN.jpg\n",
            "Saving tubo_16_ANN.jpg to tubo_16_ANN.jpg\n",
            "Saving tubo_17_ANN.jpg to tubo_17_ANN.jpg\n",
            "Saving tubo_18_ANN.jpg to tubo_18_ANN.jpg\n",
            "Saving tubo_19_ANN.jpg to tubo_19_ANN.jpg\n",
            "Saving tubo_20_ANN.jpg to tubo_20_ANN.jpg\n",
            "Saving tubo_21_ANN.jpg to tubo_21_ANN.jpg\n",
            "Saving tubo_22_ANN.jpg to tubo_22_ANN.jpg\n",
            "Saving tubo_23_ANN.jpg to tubo_23_ANN.jpg\n",
            "Saving tubo_24_ANN.jpg to tubo_24_ANN.jpg\n",
            "Saving tubo_25_ANN.jpg to tubo_25_ANN.jpg\n",
            "Saving tubo_26_ANN.jpg to tubo_26_ANN.jpg\n",
            "Saving tubo_27_ANN.jpg to tubo_27_ANN.jpg\n",
            "Saving tubo_28_ANN.jpg to tubo_28_ANN.jpg\n",
            "Saving tubo_29_ANN.jpg to tubo_29_ANN.jpg\n",
            "Saving tubo_30_ANN.jpg to tubo_30_ANN.jpg\n",
            "Saving tubo_31_ANN.jpg to tubo_31_ANN.jpg\n",
            "Saving tubo_32_ANN.jpg to tubo_32_ANN.jpg\n",
            "Saving tubo_33_ANN.jpg to tubo_33_ANN.jpg\n",
            "Saving tubo_34_ANN.jpg to tubo_34_ANN.jpg\n",
            "Saving tubo_35_ANN.jpg to tubo_35_ANN.jpg\n",
            "Saving tubo_36_ANN.jpg to tubo_36_ANN.jpg\n",
            "Saving tubo_37_ANN.jpg to tubo_37_ANN.jpg\n",
            "Saving tubo_38_ANN.jpg to tubo_38_ANN.jpg\n",
            "Saving tubo_39_ANN.jpg to tubo_39_ANN.jpg\n",
            "Saving tubo_40_ANN.jpg to tubo_40_ANN.jpg\n",
            "Saving tubo_41_ANN.jpg to tubo_41_ANN.jpg\n",
            "Saving tubo_42_ANN.jpg to tubo_42_ANN.jpg\n",
            "Saving tubo_43_ANN.jpg to tubo_43_ANN.jpg\n",
            "Saving tubo_44_ANN.jpg to tubo_44_ANN.jpg\n",
            "Saving tubo_45_ANN.jpg to tubo_45_ANN.jpg\n",
            "Saving tubo_46_ANN.jpg to tubo_46_ANN.jpg\n",
            "Saving tubo_47_ANN.jpg to tubo_47_ANN.jpg\n",
            "Saving tubo_48_ANN.jpg to tubo_48_ANN.jpg\n",
            "Saving tubo_49_ANN.jpg to tubo_49_ANN.jpg\n",
            "Saving tubo_50_ANN.jpg to tubo_50_ANN.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzo8o-JPBEA5",
        "colab_type": "code",
        "outputId": "28f112c2-3683-48bf-9225-c94d2801b91e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "img_name_list = [] #armazenarei o nome de cada imagem em uma lista\n",
        "list_img = [] # armazenarei as imagens que usarei para treinar a rede, em uma lista.\n",
        "for i in range (1, N_img + 1):\n",
        "  \n",
        "  img_name = str( 'tubo_' + str(i) + '_ANN' + '.jpg' ) #coloquei para ler manualmente o nome da imagem que eu importei da pasta de imagens que irei utilizar para treinar a rede\n",
        "  \n",
        "  img = cv2.imread(img_name) # o \"cv2.imread\" carrega a imagem, logo eu estou salvando a imagem na variavel \"img\". Obviamente se vc nao carregou a imagem, essa variavel sera do tipo None\n",
        "  \n",
        "  \n",
        "  if img is not None: \n",
        "        \n",
        "    img = cv2.resize(img,(IMG_SIZE,IMG_SIZE)) #agora que a imagem foi salva, muda-se sua resolução: para IMG_SIZE que foi declarado anteriormente, a quantidade de pixels é a multiplicação de X por Y, nesse caso, IMG_SIZE*IMG_SIZE, ou seja, o comprimento desta variável que estamos armazenando dará o valor de pixels.\n",
        "    \n",
        "    \n",
        "   \n",
        "    list_img.append(img) # armazenando as imagens de uma por uma ao longo da execução do loop, em formato de list np.array (matriz) ao invés de list\n",
        "    #as imagens são armazenadas sob a forma de números pixels\n",
        "    \n",
        "    img_name_list.append(img_name)\n",
        "    \n",
        "    \n",
        "    #training_data.append([np.array(img), np.array(label)]) , no nosso caso não temos label, pois só temos 1 classe em nossas imagens, que são as tubulações\n",
        "  else:\n",
        "    print(\"\\n\")\n",
        "    print(\"Você esqueceu de carregar a imagem \" + \"chamada \" + img_name)\n",
        "    print(\"\\n\")\n",
        "    print(\"Tente carregar todas as imagens novamente e execute novamente o programa, obrigado\")\n",
        "    print(\"\\n\")\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "  \n",
        "#Uma vez que já temos nossas imagens carregadas para treino, iremos salvar elas em uma matriz de treino  \n",
        "\n",
        "x = list_img # a matriz de treino é a própria list_img\n",
        "\n",
        "\n",
        "\n",
        "#utilizaremos np.zeros apenas para não dar problema de index inexistente na hora de alocar um valor a uma determinada posição da matriz\n",
        "y=np.zeros(len(img_name_list))   #armazenando os labels em y,  np.zeros: Return a new array of given shape and type, filled with zeros.\n",
        "for i in range(len(img_name_list)):\n",
        "  if 'tubo' in img_name_list[i]:  #se tiver a palavra \"tubo\" na imagem i, adicione o label 0 para esta imagem, o pensamento é esse...\n",
        "    y[i]=0    # para cada elemento da matriz imagem de treino, terá um label.\n",
        "  else:\n",
        "    y[i]=1    # o label 1 não vai ser dado pra nenhuma imagem pois só tem-se 50 imagens apenas de tubos, ou seja, so temos 1 classe.\n",
        "\n",
        "\n",
        "print(y)\n",
        "print('len(x) é',len(x))\n",
        "print('len(y) é',len(y))\n",
        "print(len(img_name_list))\n",
        "\n",
        "print(type(list_img))\n",
        "\n",
        "print(x[30].shape)   # a dimensão é (IMG_SIZE X IMG_SIZE X 3), o último termo equivale a 3, que significa 3 RGB (red-green-blue), cores que que compõem um pixel.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0.]\n",
            "len(x) é 50\n",
            "len(y) é 50\n",
            "50\n",
            "<class 'list'>\n",
            "(40, 40, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVYOKPhgKDZS",
        "colab_type": "code",
        "outputId": "6be82ddf-f4a9-4567-d8fb-203ae6c0fdc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os \n",
        "import cv2\n",
        "import re, random\n",
        "\n",
        "!pip install keras-tqdm\n",
        "import keras\n",
        "from tqdm import tqdm_notebook\n",
        "from random import shuffle\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import Callback\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras import layers, models, optimizers\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tqdm\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5c/ac63c65b79a895b8994474de2ad4d5b66ac0796b8903d60cfea3f8308d5c/keras_tqdm-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-tqdm) (2.2.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tqdm) (4.28.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.14.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.0.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.1.0)\n",
            "Installing collected packages: keras-tqdm\n",
            "Successfully installed keras-tqdm-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV9KyDzuJ-kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "x_train, x_test,y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1) # train_test_split: Split arrays or matrices into random train and test subsets\n",
        "#test_size = 0.2 é usado para que 20% dos dados sejam utilizados para teste . random_state=1 é apenas para escolha do gerador de numeros aleatorios da função imbutida.\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NysV2Fk-NrPd",
        "colab_type": "code",
        "outputId": "2e7a0027-c3c5-40be-b43d-976fea577b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_train=np.array(x_train) # passando a list para np.array que é o argumento que o keras e o tensor flow aceita como argumento de entrada\n",
        "\n",
        "x_test=np.array(x_test)\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "x_train = tf.image.resize_images(x_train, [32, 32])    # esse comando serve apenas para caso vc quisesse mudar a resolução das imagens 2D eixo X pelo eixo Y\n",
        "x_test = tf.image.resize_images(x_test, [32, 32])\n",
        "'''\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40, 40, 40, 3)\n",
            "(40, 40, 40, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEYCJUr0R_Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalizando os valores do pixel contidos na matriz das imagens\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True) # 255 é o valor máximo numérico numa escala de cor que o pixel de uma imagem pode assumir. o range varia de 0 até 255. 0 é preto e 255 é branco. \n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bck9ilWVSilP",
        "colab_type": "code",
        "outputId": "6e08de34-b558-4add-b8d2-b497563ce805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "#PREPARANDO A ESTRUTURA DA REDE NEURAL DE CONVOLUÇÃO\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape = (IMG_SIZE, IMG_SIZE, 3), activation = 'relu')) # essa dimensão \"3\" do lado de IMG_SIZE se refere ao RGB do pixel\n",
        "\n",
        "# Step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adding a second convolutional layer\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the CNN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Part 2 - Fitting the CNN to the images\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "\n",
        "\n",
        "'''training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
        "                                                 target_size = (IMG_SIZE, IMG_SIZE),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
        "                                            target_size = (IMG_SIZE, IMG_SIZE),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')'''\n",
        "\n",
        "batch_size = 32\n",
        "training_set = train_datagen.flow(x_train, y_train, batch_size=batch_size)\n",
        "test_set = val_datagen.flow(x_test,y_test, batch_size=batch_size)\n",
        "\n",
        "classifier.fit_generator(training_set,\n",
        "                         steps_per_epoch =10,\n",
        "                         epochs = 10,\n",
        "                         validation_data = test_set,\n",
        "                         validation_steps = 50)\n",
        "\n",
        "\n",
        "\n",
        "#COMMENTS: ->Increasing the number of layers of a neural network (NN) make it becomesa deep NN;\n",
        "# -> Convolutional NN(Convnet) are specifically to image recognition;\n",
        "#-> Convnet are inspired by the animal visual Cortex;\n",
        "#-> When you use the dropout method you remove overfitting problem, but you put the problem over far from reality, so you have to put a small rate of dropout.\n",
        "#->Example: An image has a resolution of X x Y of size, so it has X . Y pixels. Ex.: 50 x 50 = 2500 pixels;\n",
        "#-> It is needed X . Y nodes in the first layer to process each pixel;\n",
        "#Higher the number of pixels more inefficient is the training;\n",
        "\n",
        "#The components of convnets, as convolution layers, normalization layers, pooling layers are used to extract the high level features out of the images,\n",
        "# and then are feeded to a fully connected NN or any other classifier for that matter.\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 2s 210ms/step - loss: 0.1157 - acc: 0.9835 - val_loss: 1.0167e-07 - val_acc: 1.0000\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 1s 116ms/step - loss: 1.3667e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 1s 119ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 1s 116ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 1s 117ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 1s 119ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 1s 120ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 1s 119ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 1s 122ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 1s 115ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f531f3dee48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrDiDnI2Gz-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}